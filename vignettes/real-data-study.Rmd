---
title: "Real Data Study"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{real-data-study}
  %\VignetteEngine{knitr::knitr}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(knitr)
library(FOCI)
library(KPC)
devtools::install()
library(FORD)
library(ggplot2)
library(randomForest)
library(kernlab)
library(dplyr)
library(stringr)
library(purrr)
#install.packages("VIM")
library(VIM)
#install.packages("mice")
library(mice)
```

## Real Data Studies UCI

We evaluated **FORD** on three benchmark real datasets from the UCI Machine Learning Repository, comparing its performance with previous approaches such as **FOCI** [*A Simple Measure Of Conditional Dependence*](https://www.jstor.org/stable/27170947) and **KFOCI** [*Kernel Partial Correlation Coefficient*](https://www.jmlr.org/papers/volume23/21-493/21-493.pdf) (with the default exponential kernel with median bandwidth and 1-NN). For each dataset, we describe the train-test split, explain the variables, provide additional context, and cite the original scientific work that introduced or analyzed the data.

**Disclaimer**: Note that the codes has been run on a cloud computing resource with 16 cores and 128 GB RAM, and the result is used here.

1. **Superconductivity:**  
   The dataset is randomly split into 70% for training and 30% for testing. It contains 81 features extracted from 21,263 superconductors, with the *critical temperature* as the target variable in the last column. The other covariates represent various chemical and thermodynamical properties of the superconductors, available in both raw and weighted versions. The weighted versions consider the weighted mean, geometric mean, entropy, range, and standard deviation of these properties. The primary objective is to predict the critical temperature based on these features. The dataset was introduced and analyzed in [Hamidieh,2018](https://arxiv.org/pdf/1803.10260) and is publicly available on the UCI repository at [this link](https://archive.ics.uci.edu/dataset/464/superconductivty+data).

```{r load Supperconductivity code, eval=FALSE}
# Libraries ----
library(FOCI)
library(KPC)
library(randomForest)
library(kernlab)
library(ford)

# Data Preprocessing ----

## Load training data ----
filepath_train <- system.file("extdata", "supperconductivity", "suppercoductivity_data.csv", package = "ford")
suppercoductivity_data <- read.csv(filepath_train, header = TRUE)

# Train and Test Split ----
set.seed(7)
n <- nrow(suppercoductivity_data)
train_indices <- sample(seq_len(n), size = round(0.7 * n))

train_suppercoductivity <- suppercoductivity_data[train_indices, ]
test_suppercoductivity <- suppercoductivity_data[-train_indices, ]

Y_train <- train_suppercoductivity[[82]]
X_train <- train_suppercoductivity[, -82]

Y_test <- test_suppercoductivity[[82]]
X_test <- test_suppercoductivity[, -82]

# Methods to apply
methods <- list(
  foci = function(Y, X) foci(Y, X, numCores = parallel::detectCores()),
  ford = function(Y, X) ford(Y, X, numCores = parallel::detectCores()),
  kfoci = function(Y, X) KFOCI(Y, X, Knn = 1, numCores = parallel::detectCores())
)

# Store results
supperconductivity_results <- list()

# Pipeline for Feature Selection Methods ----
for (method_name in names(methods)) {

  # Feature selection
  selector <- methods[[method_name]](Y_train, X_train)
  if (method_name == "kfoci") {
    selected_idx <- selector
  } else {
    selected_idx <- selector$selectedVar$index
  }
  selected_vars <- names(X_train)[selected_idx]

  # Train Random Forest
  train_selected <- data.frame(Y = Y_train, X_train[, selected_idx, drop = FALSE])
  set.seed(7)
  model <- randomForest(Y ~ ., data = train_selected)

  # Test prediction
  test_selected <- X_test[, selected_idx, drop = FALSE]
  preds <- predict(model, newdata = test_selected)

  # Evaluation
  mse <- mean((preds - Y_test)^2)

  # Record the results
  supperconductivity_results[[method_name]] <- list(
    selected_variables = selected_vars,
    MSE = mse
  )

  #cat("Done:", method_name, "MSE =", mse, "\n")
}

# Random Forest benchmark (no feature selection) ----
set.seed(7)
rf_model_full <- randomForest(Y_train ~ ., data = data.frame(Y_train = Y_train, X_train))

# Predictions and MSE
rf_preds_full <- predict(rf_model_full, newdata = X_test)
rf_mse_full <- mean((rf_preds_full - Y_test)^2)

# Feature importance ranking
importance_scores <- importance(rf_model_full)
ordered_vars <- names(sort(importance_scores[, 1], decreasing = TRUE))  # IncMSE column

# Record benchmark result
supperconductivity_results[["random_forest_full"]] <- list(
  selected_variables = ordered_vars,
  MSE = rf_mse_full
)

#cat("Done: Random Forest Full Model, MSE =", rf_mse_full, "\n")

# Final Results ----
#save(supperconductivity_results, file = "/results/supperconductivity_results.RData")
```


2. **Wave Energy Converter:**  
   The dataset is randomly split into 70% for training and 30% for testing. It includes the positions and absorbed power outputs of wave energy converters (WECs) in real wave scenarios from "Tasmania," the southern coast of Australia. The dataset has 72000 rows and features 16 variables for the positions of the WECs, denoted as X1, X2, ..., X16; Y1, Y2, ..., Y16, and 16 variables for the absorbed power outputs, denoted as P1, P2, ..., P16. The target variable is the total power output of the farm, referred to as *Powerall*. The objective is to predict the total power output based on the positions and power outputs of individual WECs. This dataset and its application were discussed in [Neshat,2019](https://www.academia.edu/62902910/A_new_insight_into_the_Position_Optimization_of_Wave_Energy_Converters_by_a_Hybrid_Local_Search), and it is available at the UCI repository through [this link](https://archive.ics.uci.edu/dataset/494/wave+energy+converters).

```{r load Wave Energy Converter code, eval=FALSE}
# Libraries ----
library(FOCI)
library(KPC)
library(randomForest)
library(kernlab)
library(ford)

# Data Preprocessing ----

## Load data ----
city_names <- c("Adelaide", "Perth", "Sydney", "Tasmania")
wec_datasets <- list()

for (city in city_names) {
  filepath <- system.file("extdata", "wave_energy_converters", paste0(city, "_Data.csv"), package = "ford")
  data <- read.csv(filepath, header = FALSE)
  wec_datasets[[city]] <- list(train = NULL, test = NULL, data = data)
}

# Split datasets into train/test
set.seed(7)
for (name in names(wec_datasets)) {
  n <- nrow(wec_datasets[[name]]$data)
  train_indices <- sample(seq_len(n), size = round(0.7 * n))
  wec_datasets[[name]]$train <- wec_datasets[[name]]$data[train_indices, ]
  wec_datasets[[name]]$test <- wec_datasets[[name]]$data[-train_indices, ]
}

# Methods to apply
methods <- list(
  foci = function(Y, X) foci(Y, X, numCores = parallel::detectCores()),
  ford = function(Y, X) ford(Y, X, numCores = parallel::detectCores()),
  kfoci = function(Y, X) KFOCI(Y, X, Knn = 1, numCores = parallel::detectCores())
)

# Store results
wec_results <- list()

# Pipeline ----
for (dataset_name in names(wec_datasets)) {
  dataset <- wec_datasets[[dataset_name]]
  train_data <- dataset$train
  test_data <- dataset$test

  Y_train <- train_data[[49]]
  X_train <- train_data[, -49]

  Y_test <- test_data[[49]]
  X_test <- test_data[, -49]

  for (method_name in names(methods)) {

    # Feature selection
    selector <- methods[[method_name]](Y_train, X_train)
    if (method_name == "kfoci") {
      selected_idx <- selector
    } else {
      selected_idx <- selector$selectedVar$index
    }
    selected_vars <- names(X_train)[selected_idx]

    # Train Random Forest
    train_selected <- data.frame(Y = Y_train, X_train[, selected_idx, drop = FALSE])
    set.seed(7)
    model <- randomForest(Y ~ ., data = train_selected)

    # Test prediction
    test_selected <- X_test[, selected_idx, drop = FALSE]
    preds <- predict(model, newdata = test_selected)

    # Evaluation
    mse <- mean((preds - Y_test)^2)

    # Record the results
    wec_results[[paste(dataset_name, method_name, sep = "_")]] <- list(
      selected_variables = selected_vars,
      MSE = mse
    )

    #cat("Done:", dataset_name, method_name, "MSE =", mse, "\n")
  }

  # Random Forest benchmark (no feature selection) ----
  set.seed(7)
  rf_model_full <- randomForest(Y_train ~ ., data = data.frame(Y_train = Y_train, X_train))

  rf_preds_full <- predict(rf_model_full, newdata = X_test)
  rf_mse_full <- mean((rf_preds_full - Y_test)^2)

  # Feature importance
  importance_scores <- importance(rf_model_full)
  ordered_vars <- names(sort(importance_scores[, 1], decreasing = TRUE))  # IncMSE

  wec_results[[paste(dataset_name, "random_forest_full", sep = "_")]] <- list(
    selected_variables = ordered_vars,
    MSE = rf_mse_full
  )

  #cat("Done:", dataset_name, "Random Forest Full Model, MSE =", rf_mse_full, "\n")
}

# Final Results ----
#save(wec_results, file = "/results/wec_results.RData")

```


3. **Lattice Physics:**  
   This dataset includes a training set containing 23,999 observations and a test set with 359 observations. Each observation represents a distinct fuel enrichment configuration for a NuScale US600 fuel assembly type C-01 (NFAC-01). The dataset includes 39 features corresponding to U-235 enrichment levels, ranging from 0.7 to 5.0 weight percent, for fuel rods within a one-eighth symmetric segment of the assembly. The response variable of interest is the infinite multiplication factor (*k-inf*) which is computed using the MCNP6 Monte Carlo simulation code. The task is to predict the performance indicator *k-inf* based on the fuel rod enrichment levels. The simulation results and dataset were generated and presented in [Nguyen Huu Tiep et al. 2024](https://www.mdpi.com/2227-7390/12/24/3892), and can be accessed via [this link](https://archive.ics.uci.edu/dataset/1091/lattice-physics+(pwr+fuel+assembly+neutronics+simulation+results)).

```{r load Lattice Physics result, eval=FALSE}
# Libraries ----
library(FOCI)
library(KPC)
library(randomForest)
library(kernlab)
library(ford)

# Data Preprocessing ----

## Load training data ----
filepath_train <- system.file("extdata", "lattice_physics", "lattice_physics_train.csv", package = "ford")
lattice_physics_train <- read.table(filepath_train, header = FALSE)
colnames(lattice_physics_train) <- c("k-inf", "PPPF", paste0("C", 1:39))
X_train <- lattice_physics_train[, 3:41]

## Load test data ----
filepath_test <- system.file("extdata", "lattice_physics", "lattice_physics_test.csv", package = "ford")
lattice_physics_test <- read.table(filepath_test, header = FALSE)
colnames(lattice_physics_test) <- c("k-inf", "PPPF", paste0("C", 1:39))
X_test <- lattice_physics_test[, 3:41]

# Responses
responses <- c("k-inf", "PPPF")

# Methods to apply
methods <- list(
  foci = function(Y, X) foci(Y, X, numCores = 1),
  ford = function(Y, X) ford(Y, X, numCores = 1),
  kfoci = function(Y, X) KFOCI(Y, X, Knn = 1, numCores = 1)
)

# Store results
lattice_physics_results <- list()

# Pipeline ----
for (resp in responses) {
  Y_train <- lattice_physics_train[[resp]]
  Y_test <- lattice_physics_test[[resp]]

  for (method_name in names(methods)) {

    # Feature selection
    selector <- methods[[method_name]](Y_train, X_train)
    if (method_name == "kfoci") {
      selected_idx <- selector
    } else {
      selected_idx <- selector$selectedVar$index
    }
    selected_vars <- names(X_train)[selected_idx]

    # Train Random Forest
    train_data <- data.frame(Y = Y_train, X_train[, selected_idx, drop = FALSE])
    set.seed(1)
    model <- randomForest(Y ~ ., data = train_data)

    # Test prediction
    test_data <- X_test[, selected_idx, drop = FALSE]
    preds <- predict(model, newdata = test_data)

    # Evaluation
    mse <- mean((preds - Y_test)^2)

    # Record the results
    lattice_physics_results[[paste(resp, method_name, sep = "_")]] <- list(
      selected_variables = selected_vars,
      MSE = mse
    )

    #cat("Done:", resp, method_name, "MSE =", mse, "\n")
  }

  # Random Forest benchmark (no feature selection) ----
  set.seed(1)
  rf_model_full <- randomForest(Y_train ~ ., data = data.frame(Y_train = Y_train, X_train))

  # Predictions and MSE
  rf_preds_full <- predict(rf_model_full, newdata = X_test)
  rf_mse_full <- mean((rf_preds_full - Y_test)^2)

  # Feature importance ranking
  importance_scores <- importance(rf_model_full)
  ordered_vars <- names(sort(importance_scores[, 1], decreasing = TRUE))  # IncMSE column

  # Record benchmark result
  lattice_physics_results[[paste(resp, "random_forest_full", sep = "_")]] <- list(
    selected_variables = ordered_vars,
    MSE = rf_mse_full
  )

  #cat("Done:", resp, "Random Forest Full Model, MSE =", rf_mse_full, "\n")
}

# Final Results ----
#save(lattice_physics_results, file = "/results/lattice_physics_results.RData")
```


FORD was compared with the competitors FOCI and KFOCI (with the default exponential kernel with median bandwidth and 1-NN). For each method, after selecting the variables, a predictive model
was fitted to a training set using random forests. Mean squared prediction errors (MSPE) were estimated using a
test set as specified for each data set in the explanation. The sizes of the selected subsets and the MSPEs are reported in the following
Tables. In all three examples, FORD attained comparative prediction errors as
the other methods.

### Results

```{r unified-results-table, message=FALSE, warning=FALSE}

# Helper to safely load and extract method info
extract_results <- function(res, foci_key, kfoci_key, ford_key, rf_key) {
  methods <- c(foci_key, kfoci_key, ford_key)
  subset_sizes <- c(length(res[[foci_key]]$selected_variables),
                    length(res[[kfoci_key]]$selected_variables),
                    length(res[[ford_key]]$selected_variables),
                    length(res[[rf_key]]$selected_variables))
  mspe <- c(res[[foci_key]]$MSE,
            res[[kfoci_key]]$MSE,
            res[[ford_key]]$MSE,
            res[[rf_key]]$MSE)
  list(subset_sizes = subset_sizes, mspe = mspe)
}

# Load Superconductivity results
result_path_sc <- system.file("extdata","supperconductivity_results.RData", package = "FORD")
load(result_path_sc)
sc_res <- extract_results(supperconductivity_results, "foci", "kfoci", "ford","random_forest_full")

# Load WEC results
result_path_sc <- system.file("extdata","wec_results.RData", package = "FORD")
load(result_path_sc)
wec_res <- extract_results(wec_results, "Tasmania_foci", "Tasmania_kfoci", "Tasmania_ford","Tasmania_random_forest_full")

# Load Lattice Physics results
result_path_sc <- system.file("extdata","lattice_physics_results.RData", package = "FORD")
load(result_path_sc)
lp_res <- extract_results(lattice_physics_results, "k-inf_foci", "k-inf_kfoci", "k-inf_ford","k-inf_random_forest_full")

# Build full table
methods <- c("FOCI", "KFOCI", "FORD")
final_df_1 <- data.frame(
  Method = methods,
  SC_Subset = sc_res$subset_sizes[1:3],
  SC_MSPE = formatC(sc_res$mspe[1:3], format = "e", digits = 2),
  WEC_Subset = wec_res$subset_sizes[1:3],
  WEC_MSPE = formatC(wec_res$mspe[1:3], format = "e", digits = 2),
  LP_Subset = lp_res$subset_sizes[1:3],
  LP_MSPE = formatC(lp_res$mspe[1:3], format = "e", digits = 2)
)

# Display
kable(final_df_1, caption = "Applications of FOCI, KFOCI, and FORD to Real Datasets", digits = 2)

# Build full table
datasets_names <- c("SC", "WEC", "LP")

ford_subset_sizes <- c(sc_res$subset_sizes[3], wec_res$subset_sizes[3], lp_res$subset_sizes[3])
rf_subset_sizes <- c(sc_res$subset_sizes[4], wec_res$subset_sizes[4], lp_res$subset_sizes[4])

ford_mspe <- c(formatC(sc_res$mspe[3], format = "e", digits = 2),
               formatC(wec_res$mspe[3], format = "e", digits = 2),
               formatC(lp_res$mspe[3], format = "e", digits = 2))

rf_mspe <- c(formatC(sc_res$mspe[4], format = "e", digits = 2),
               formatC(wec_res$mspe[4], format = "e", digits = 2),
               formatC(lp_res$mspe[4], format = "e", digits = 2))

final_df_2 <- data.frame(
  Data = datasets_names,
  Subset_ratio = paste0(ford_subset_sizes,"/",rf_subset_sizes),
  FORD_MSPE = ford_mspe,
  RF_MSPE = rf_mspe
)

# Display
kable(final_df_2, caption = "Applications of FOCI, KFOCI, and FORD to Real Datasets", digits = 2)
```




